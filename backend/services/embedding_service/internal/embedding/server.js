import express from "express";
import bodyParser from "body-parser";
import { pipeline, env } from "@xenova/transformers";

// Model cache ayarlarÄ±
env.allowLocalModels = false;
env.useBrowserCache = false;

const PORT = process.env.PORT || 3000;
const app = express();
app.use(bodyParser.json({ limit: "10mb" }));

let embedModel = null;
let modelLoading = false;

async function loadModel() {
  if (modelLoading) {
    console.log("Model already loading...");
    return;
  }

  modelLoading = true;

  try {
    console.log("ğŸ“¦ Loading Xenova embedding model...");
    console.log("ğŸ“¦ Model: Xenova/all-MiniLM-L6-v2");

    // "feature-extraction" kullan - bu kesin Ã§alÄ±ÅŸÄ±r
    embedModel = await pipeline("embeddings", "Xenova/all-MiniLM-L6-v2");

    console.log("âœ… Model loaded successfully!");
    modelLoading = false;
    return embedModel;
  } catch (error) {
    console.error("âŒ Model loading failed:", error.message);
    console.error("Stack:", error.stack);
    modelLoading = false;
    throw error;
  }
}

app.post("/embed", async (req, res) => {
  try {
    const { text } = req.body;

    if (!text) {
      return res.status(400).json({ error: "text field is required" });
    }

    if (!embedModel) {
      return res.status(503).json({
        error: "model is still loading",
        loading: modelLoading,
      });
    }

    // Embedding oluÅŸtur
    const output = await embedModel(text, {
      pooling: "mean",
      normalize: true,
    });

    // Output her zaman tensor formatÄ±nda gelir
    // .data ile raw array'e Ã§evir
    const vector = Array.from(output.data);

    console.log(`ğŸ“Š Generated embedding with dimension: ${vector.length}`);

    res.json({
      embedding: vector,
      dimension: vector.length,
    });
  } catch (err) {
    console.error("âŒ Embedding error:", err);
    res.status(500).json({
      error: err.message || "internal server error",
    });
  }
});

app.get("/health", (req, res) => {
  if (embedModel) {
    res.json({ status: "ready", model: "Xenova/all-MiniLM-L6-v2" });
  } else if (modelLoading) {
    res.json({ status: "loading", model: "Xenova/all-MiniLM-L6-v2" });
  } else {
    res.status(503).json({ status: "not_ready" });
  }
});

// Test endpoint
app.get("/test", async (req, res) => {
  try {
    if (!embedModel) {
      return res.status(503).json({ error: "model not ready" });
    }

    const testText = "Hello world";
    const output = await embedModel(testText, {
      pooling: "mean",
      normalize: true,
    });
    const vector = Array.from(output.data);

    res.json({
      test: "success",
      text: testText,
      dimension: vector.length,
      sample: vector.slice(0, 5), // Ä°lk 5 deÄŸeri gÃ¶ster
    });
  } catch (err) {
    res.status(500).json({ error: err.message });
  }
});

// Server'Ä± baÅŸlat
async function startServer() {
  try {
    await loadModel();

    app.listen(PORT, "0.0.0.0", () => {
      console.log(`ğŸš€ Xenova embedding server running on port ${PORT}`);
      console.log(`ğŸ“ Health check: http://localhost:${PORT}/health`);
      console.log(`ğŸ“ Test endpoint: http://localhost:${PORT}/test`);
    });
  } catch (error) {
    console.error("ğŸ’¥ Failed to start server:", error);
    process.exit(1);
  }
}

startServer();
